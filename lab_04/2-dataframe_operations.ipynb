{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Operations\n",
    "\n",
    "## About the Data\n",
    "In this notebook, we will be working with 2 datasets:\n",
    "- Facebook's stock price throughout 2018 (obtained using the [`stock_analysis` package](https://github.com/fenago/stock-analysis)).\n",
    "- Daily weather data for NYC from the [National Centers for Environmental Information (NCEI) API](https://www.ncdc.noaa.gov/cdo-web/webservices/v2).\n",
    "\n",
    "*Note: The NCEI is part of the National Oceanic and Atmospheric Administration (NOAA) and, as you can see from the URL for the API, this resource was created when the NCEI was called the NCDC. Should the URL for this resource change in the future, you can search for \"NCEI weather API\" to find the updated one.*\n",
    "\n",
    "## Background on the weather data\n",
    "\n",
    "Data meanings:\n",
    "- `AWND`: average wind speed\n",
    "- `PRCP`: precipitation in millimeters\n",
    "- `SNOW`: snowfall in millimeters\n",
    "- `SNWD`: snow depth in millimeters\n",
    "- `TMAX`: maximum daily temperature in Celsius\n",
    "- `TMIN`: minimum daily temperature in Celsius\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "weather = pd.read_csv('data/nyc_weather_2018.csv', parse_dates=['date'])\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb = pd.read_csv('data/fb_2018.csv', index_col='date', parse_dates=True)\n",
    "fb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic and statistics\n",
    "We already saw that we can use mathematical operators like `+` and `/` with dataframes directly. However, we can also use methods, which allow us to specify the axis to perform the calculation over. By default, this is per column. Let's find the Z-scores for the volume traded and look at the days where this was more than 3 standard deviations from the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.assign(\n",
    "    abs_z_score_volume=lambda x: \\\n",
    "        x.volume.sub(x.volume.mean()).div(x.volume.std()).abs()\n",
    ").query('abs_z_score_volume > 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `rank()` and `pct_change()` to see which days had the largest change in volume traded from the day before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.assign(\n",
    "    volume_pct_change=fb.volume.pct_change(),\n",
    "    pct_change_rank=lambda x: \\\n",
    "        x.volume_pct_change.abs().rank(ascending=False)\n",
    ").nsmallest(5, 'pct_change_rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "January 12th was when the news that Facebook changed its news feed product to focus more on content from a users' friends over the brands they follow. Given that Facebook's advertising is a key component of its business ([nearly 89% in 2017](https://www.investopedia.com/ask/answers/120114/how-does-facebook-fb-make-money.asp)), many shares were sold and the price dropped in panic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['2018-01-11':'2018-01-12']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout 2018, Facebook's stock price never had a low above $215:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fb > 215).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facebook's OHLC (open, high, low, and close) prices all had at least one day they were at $215 or less:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fb > 215).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning\n",
    "When working with volume traded, we may be interested in ranges of volume rather than the exact values. No two days have the same volume traded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fb.volume.value_counts() > 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `pd.cut()` to create 3 bins of even range in volume traded and name them. Then we can work with low, medium, and high volume traded categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_binned = pd.cut(fb.volume, bins=3, labels=['low', 'med', 'high'])\n",
    "volume_binned.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the days with high trading volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb[volume_binned == 'high'].sort_values('volume', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "July 25th Facebook announced disappointing user growth and the stock tanked in the after hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['2018-07-25':'2018-07-26']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambridge Analytica scandal broke on Saturday, March 17th, so we look at the Monday after for the numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['2018-03-16':'2018-03-20']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most days have similar volume, but a few are very large, we have very wide bins. Most of the data is in the low bin. \n",
    "\n",
    "*Note: visualizations will be covered in the next labs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_aids.misc_viz import low_med_high_bins_viz\n",
    "\n",
    "low_med_high_bins_viz(\n",
    "    fb, 'volume', ylabel='volume traded',\n",
    "    title='Daily Volume Traded of Facebook Stock in 2018 (with bins)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we split using quantiles, the bins will have roughly the same number of observations. For this, we use `qcut()`. We will make 4 quartiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_qbinned = pd.qcut(fb.volume, q=4, labels=['q1', 'q2', 'q3', 'q4'])\n",
    "volume_qbinned.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the bins don't cover ranges of the same size anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_aids.misc_viz import quartile_bins_viz\n",
    "\n",
    "quartile_bins_viz(\n",
    "    fb, 'volume', ylabel='volume traded', \n",
    "    title='Daily Volume Traded of Facebook Stock in 2018 (with quartile bins)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Functions\n",
    "We can use the `apply()` method to run the same operation on all columns (or rows) of the dataframe. First, let's isolate the weather observations from the Central Park station and pivot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_park_weather = weather\\\n",
    "    .query('station == \"GHCND:USW00094728\"')\\\n",
    "    .pivot(index='date', columns='datatype', values='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the Z-scores of the TMIN, TMAX, and PRCP observations in Central Park in October 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oct_weather_z_scores = central_park_weather\\\n",
    "    .loc['2018-10', ['TMIN', 'TMAX', 'PRCP']]\\\n",
    "    .apply(lambda x: x.sub(x.mean()).div(x.std()))\n",
    "oct_weather_z_scores.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "October 27th rained much more than the rest of the days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oct_weather_z_scores.query('PRCP > 3').PRCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, this day was much higher than the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_park_weather.loc['2018-10', 'PRCP'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the function we want to apply isn't vectorized, we can:\n",
    "- use `np.vectorize()` to vectorize it (similar to how `map()` works) and then use it with `apply()`\n",
    "- use `applymap()` and pass it the non-vectorized function directly\n",
    "\n",
    "Say we wanted to count the digits of the whole numbers for the Facebook data; `len()` is not vectorized, so we can use `np.vectorize()` or `applymap()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.apply(\n",
    "    lambda x: np.vectorize(lambda y: len(str(np.ceil(y))))(x)\n",
    ").astype('int64').equals(\n",
    "    fb.applymap(lambda x: len(str(np.ceil(x))))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple operation of addition to each element in a series grows linearly in time complexity when using `iteritems()`, but stays near 0 when using vectorized operations. `iteritems()` and related methods should only be used if there is no vectorized solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "vectorized_results = {}\n",
    "iteritems_results = {}\n",
    "\n",
    "for size in [10, 100, 1000, 10000, 100000, 500000, 1000000, 5000000, 10000000]:\n",
    "    # set of numbers to use\n",
    "    test = pd.Series(np.random.uniform(size=size))\n",
    "    \n",
    "    # time the vectorized operation\n",
    "    start = time.time()\n",
    "    x = test + 10\n",
    "    end = time.time()\n",
    "    vectorized_results[size] = end - start\n",
    "    \n",
    "    # time the operation with `iteritems()`\n",
    "    start = time.time()\n",
    "    x = []\n",
    "    for i, v in test.iteritems():\n",
    "        x.append(v + 10)\n",
    "    x = pd.Series(x)\n",
    "    end = time.time()\n",
    "    iteritems_results[size] = end - start\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    [pd.Series(vectorized_results, name='vectorized'), pd.Series(iteritems_results, name='iteritems')]\n",
    ").T    \n",
    "\n",
    "# plotting\n",
    "ax = results.plot(title='Time Complexity', color=['blue', 'red'], legend=False)\n",
    "\n",
    "# formatting\n",
    "ax.set(xlabel='item size (rows)', ylabel='time (s)')\n",
    "ax.text(0.5e7, iteritems_results[0.5e7] * .9, 'iteritems()', rotation=34, color='red', fontsize=12, ha='center', va='bottom')\n",
    "ax.text(0.5e7, vectorized_results[0.5e7], 'vectorized', color='blue', fontsize=12, ha='center', va='bottom')\n",
    "for spine in ['top', 'right']:\n",
    "    ax.spines[spine].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Calculations\n",
    "*Consult the [`understanding_window_calculations.ipynb`](./understanding_window_calculations.ipynb) notebook for interactive visualizations using widgets to help understand window calculations.*\n",
    "\n",
    "The `rolling()` method allows us to perform rolling window calculations. We simply specify the window size (3 days here) and follow it with a call to an aggregation function (sum here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_park_weather.loc['2018-10'].assign(\n",
    "    rolling_PRCP=lambda x: x.PRCP.rolling('3D').sum()\n",
    ")[['PRCP', 'rolling_PRCP']].head(7).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform the rolling calculations on the entire dataframe at once. This will apply the same aggregation function to each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_park_weather.loc['2018-10'].rolling('3D').mean().head(7).iloc[:,:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use different aggregation functions per column if we use `agg()` instead. We pass in a dictionary mapping the column to the aggregation to perform on it. Here, we join the result to the original data to see what is happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_park_weather['2018-10-01':'2018-10-07'].rolling('3D').agg(\n",
    "    {'TMAX': 'max', 'TMIN': 'min', 'AWND': 'mean', 'PRCP': 'sum'}\n",
    ").join( # join with original data for comparison\n",
    "    central_park_weather[['TMAX', 'TMIN', 'AWND', 'PRCP']], \n",
    "    lsuffix='_rolling'\n",
    ").sort_index(axis=1) # sort columns so rolling calcs are next to originals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we reindexed the Facebook stock data as we did with the S&P 500 data in lab 3. If we were to use rolling calculations on this data, we would be including the values when the market was closed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_reindexed = fb\\\n",
    "    .reindex(pd.date_range('2018-01-01', '2018-12-31', freq='D'))\\\n",
    "    .assign(\n",
    "        volume=lambda x: x.volume.fillna(0),\n",
    "        close=lambda x: x.close.fillna(method='ffill'),\n",
    "        open=lambda x: x.open.combine_first(x.close),\n",
    "        high=lambda x: x.high.combine_first(x.close),\n",
    "        low=lambda x: x.low.combine_first(x.close)\n",
    "    )\n",
    "fb_reindexed.assign(day=lambda x: x.index.day_name()).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of version 1.0, `pandas` supports defining custom windows for rolling calculations, which makes it possible for us to perform rolling calculations on the days the market was open. One way is to make a new class that inherits from `BaseIndexer` and provide the logic for determining the window bounds in the `get_window_bounds()` method (more info [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#custom-window-rolling)). For our use case, we can use the `VariableOffsetWindowIndexer` class, which was introduced in version 1.1, to perform rolling calculations over non-fixed time offsets (like business days). Let's perform a three business day rolling calculation on the reindexed Facebook stock data and join it with the reindexed data for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.indexers import VariableOffsetWindowIndexer\n",
    "\n",
    "indexer = VariableOffsetWindowIndexer(\n",
    "    index=fb_reindexed.index, offset=pd.offsets.BDay(3)\n",
    ")\n",
    "fb_reindexed.assign(window_start_day=0).rolling(indexer).agg({\n",
    "    'window_start_day': lambda x: x.index.min().timestamp(),\n",
    "    'open': 'mean', 'high': 'max', 'low': 'min',\n",
    "    'close': 'mean', 'volume': 'sum'\n",
    "}).join(\n",
    "    fb_reindexed, lsuffix='_rolling'\n",
    ").sort_index(axis=1).assign(\n",
    "    day=lambda x: x.index.day_name(),\n",
    "    window_start_day=lambda x: pd.to_datetime(x.window_start_day, unit='s')\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolling calculations (`rolling()`) use a sliding window. Expanding calculations (`expanding()`), however, grow in size. These are equivalent to cumulative aggregations like `cumsum()`; however, we can specify the minimum number of periods required to start calculating (default is 1), and we aren't limited to predefined aggregations. Therefore, while there is no method for the cumulative mean, we can calculate it using `expanding()`. Let's calculate the month-to-date average precipiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_park_weather.loc['2018-06'].assign(\n",
    "    TOTAL_PRCP=lambda x: x.PRCP.cumsum(),\n",
    "    AVG_PRCP=lambda x: x.PRCP.expanding().mean()\n",
    ").head(10)[['PRCP', 'TOTAL_PRCP', 'AVG_PRCP']].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `agg()` to specify aggregations per column. Note that this works with NumPy functions as well. Here, we join the expanding calculations with the original results for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_park_weather['2018-10-01':'2018-10-07'].expanding().agg(\n",
    "    {'TMAX': np.max, 'TMIN': np.min, 'AWND': np.mean, 'PRCP': np.sum}\n",
    ").join(\n",
    "    central_park_weather[['TMAX', 'TMIN', 'AWND', 'PRCP']], \n",
    "    lsuffix='_expanding'\n",
    ").sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides the `ewm()` method for exponentially weighted moving calculations. As we saw in lab 1, we can use the exponentially weighted moving average to smooth the data. Let's compare the rolling mean to the exponentially weighted moving average with the maximum daily temperature. Note that `span` here is the periods to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_park_weather.assign(\n",
    "    AVG=lambda x: x.TMAX.rolling('30D').mean(),\n",
    "    EWMA=lambda x: x.TMAX.ewm(span=30).mean()\n",
    ").loc['2018-09-29':'2018-10-08', ['TMAX', 'EWMA', 'AVG']].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Consult the [`understanding_window_calculations.ipynb`](./understanding_window_calculations.ipynb) notebook for interactive visualizations to help understand window calculations.*\n",
    "\n",
    "## Pipes\n",
    "Pipes are a way to streamline our `pandas` code and make it more readable and flexible. Using pipes, we can take a nested call like \n",
    "\n",
    "```python\n",
    "f(g(h(data), 20), x=True)\n",
    "```\n",
    "\n",
    "and turn it into something more readable:\n",
    "\n",
    "```python\n",
    "data.pipe(h)\\\n",
    "    .pipe(g, 20)\\\n",
    "    .pipe(f, x=True)\\\n",
    "```\n",
    "\n",
    "We can use pipes to apply any function that accepts our data as the first argument and pass in any additional arguments. This makes it easy to chain steps together regardless of whether they are methods or functions:\n",
    "\n",
    "We can pass any function that will accept the caller of `pipe()` as the first argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(df):\n",
    "    return '%d rows, %d columns and max closing Z-score was %d' % (*df.shape, df.close.max())\n",
    "\n",
    "get_info(fb.loc['2018-Q1'].apply(lambda x: (x - x.mean())/x.std()))\\\n",
    "    == fb.loc['2018-Q1'].apply(lambda x: (x - x.mean())/x.std()).pipe(get_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, passing `pd.DataFrame.rolling` to `pipe()` is equivalent to calling `rolling()` directly on the dataframe, except we have more flexiblity to change this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.pipe(pd.DataFrame.rolling, '20D').mean().equals(fb.rolling('20D').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipe takes the function passed in and calls it with the object that called `pipe()` as the first argument. Positional and keyword arguments are passed down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.rolling(fb, '20D').mean().equals(fb.rolling('20D').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a pipe to make a function that we can use for all of our window calculation needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from window_calc import window_calc\n",
    "window_calc??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same interface to calculate various window calculations now. Let's find the expanding median for the Facebook data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_calc(fb, pd.DataFrame.expanding, np.median).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the exponentially weighted moving average requires we pass in a keyword argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_calc(fb, pd.DataFrame.ewm, 'mean', span=3).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With rolling calculations, we can pass in a positional argument for the window size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_calc(\n",
    "    central_park_weather.loc['2018-10'], \n",
    "    pd.DataFrame.rolling, \n",
    "    {'TMAX': 'max', 'TMIN': 'min', 'AWND': 'mean', 'PRCP': 'sum'},\n",
    "    '3D'\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div style=\"overflow: hidden; margin-bottom: 10px;\">\n",
    "    <div style=\"float: left;\">\n",
    "         <a href=\"./1-querying_and_merging.ipynb\">\n",
    "            <button>&#8592; Previous Notebook</button>\n",
    "        </a>\n",
    "        <a href=\"./understanding_window_calculations.ipynb\">\n",
    "            <button>Understanding Window Calculations</button>\n",
    "        </a>\n",
    "    </div>\n",
    "    <div style=\"float: right;\">\n",
    "        <a href=\"./3-aggregations.ipynb\">\n",
    "            <button>Next Notebook &#8594;</button>\n",
    "        </a>\n",
    "    </div>\n",
    "</div>\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
