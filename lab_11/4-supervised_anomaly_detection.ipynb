{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised methods\n",
    "Since we now have labeled data, we can try some supervised methods. Our stakeholders are looking for a model with recall of at least 70% and precision of at least 85%.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "\n",
    "with sqlite3.connect('logs/logs.db') as conn:\n",
    "    logs_2018 = pd.read_sql(\n",
    "        'SELECT * FROM logs WHERE datetime BETWEEN \"2018-01-01\" AND \"2019-01-01\";', \n",
    "        conn, parse_dates=['datetime'], index_col='datetime'\n",
    "    )\n",
    "    hackers_2018 = pd.read_sql(\n",
    "        'SELECT * FROM attacks WHERE start BETWEEN \"2018-01-01\" AND \"2019-01-01\";', \n",
    "        conn, parse_dates=['start', 'end']\n",
    "    ).assign(\n",
    "        duration=lambda x: x.end - x.start, \n",
    "        start_floor=lambda x: x.start.dt.floor('min'),\n",
    "        end_ceil=lambda x: x.end.dt.ceil('min')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X(log, day):\n",
    "    \"\"\"\n",
    "    Get data we can use for the X\n",
    "    \n",
    "    Parameters:\n",
    "        - log: The logs dataframe\n",
    "        - day: A day or single value we can use as a datetime index slice\n",
    "    \n",
    "    Returns: \n",
    "        A `pandas.DataFrame` object\n",
    "    \"\"\"\n",
    "    return pd.get_dummies(log.loc[day].assign(\n",
    "        failures=lambda x: 1 - x.success\n",
    "    ).query('failures > 0').resample('1min').agg(\n",
    "        {'username': 'nunique', 'failures': 'sum'}\n",
    "    ).dropna().rename(\n",
    "        columns={'username': 'usernames_with_failures'}\n",
    "    ).assign(\n",
    "        day_of_week=lambda x: x.index.dayofweek, \n",
    "        hour=lambda x: x.index.hour\n",
    "    ).drop(columns=['failures']), columns=['day_of_week', 'hour'])\n",
    "\n",
    "def get_y(datetimes, hackers, resolution='1min'):\n",
    "    \"\"\"\n",
    "    Get data we can use for the y (whether or not a hacker attempted a log in during that time).\n",
    "    \n",
    "    Parameters:\n",
    "        - datetimes: The datetimes to check for hackers\n",
    "        - hackers: The dataframe indicating when the attacks started and stopped\n",
    "        - resolution: The granularity of the datetime. Default is 1 minute.\n",
    "        \n",
    "    Returns:\n",
    "        `pandas.Series` of Booleans.\n",
    "    \"\"\"\n",
    "    date_ranges = hackers.apply(\n",
    "        lambda x: pd.date_range(x.start_floor, x.end_ceil, freq=resolution), \n",
    "        axis=1\n",
    "    )\n",
    "    dates = pd.Series(dtype='object')\n",
    "    for date_range in date_ranges:\n",
    "        dates = pd.concat([dates, date_range.to_series()])\n",
    "    return datetimes.isin(dates)\n",
    "\n",
    "def get_X_y(log, day, hackers):\n",
    "    \"\"\"\n",
    "    Get the X, y data to build a model with.\n",
    "    \n",
    "    Parameters:\n",
    "        - log: The logs dataframe\n",
    "        - day: A day or single value we can use as a datetime index slice\n",
    "        - hackers: The dataframe indicating when the attacks started and stopped\n",
    "        \n",
    "    Returns:\n",
    "        X, y tuple where X is a `pandas.DataFrame` object\n",
    "        and y is a `pandas.Series` object\n",
    "    \"\"\"\n",
    "    X = get_X(log, day)\n",
    "    y = get_y(X.reset_index().datetime, hackers)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train on January 2018 and test on February 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_X_y(logs_2018, '2018-01', hackers_2018)\n",
    "X_test, y_test = get_X_y(logs_2018, '2018-02', hackers_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised anomaly detection\n",
    "Now that we have our labeled data, let's use supervised learning algorithms to see if we can use this extra information to find the hackers. We will build two baseline models first:\n",
    "\n",
    "- a dummy classifier that will predict labels based on the stratification in the data\n",
    "- a Naive Bayes model that will predict the labels using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselining\n",
    "#### Dummy classifier\n",
    "This classifier will give us a model that is equivalent to the baseline we have been drawing on our ROC curves. The results will be poor on purpose. We will never use this classifier to actually make predictions&mdash;rather, we can use it to see if the models we are building are better than random guessing strategies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier # build a baseline\n",
    "\n",
    "dummy_model = DummyClassifier(strategy='stratified', random_state=0).fit(X_train, y_train)\n",
    "dummy_preds = dummy_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Partials\n",
    "Create partials for less typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create partials for less typing\n",
    "from functools import partial\n",
    "from sklearn.metrics import classification_report\n",
    "from ml_utils.classification import confusion_matrix_visual, plot_pr_curve, plot_roc\n",
    "\n",
    "report = partial(classification_report, y_test)\n",
    "roc = partial(plot_roc, y_test)\n",
    "pr_curve = partial(plot_pr_curve, y_test)\n",
    "conf_matrix = partial(confusion_matrix_visual, y_test, class_labels=[False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No attackers are properly flagged with the dummy model. Notice how the ROC curve is diagonal and on top of the baseline we have seen in previous chapters. The precision-recall curve is also along its baseline at the percentage of attackers in the data. This means the dummy model is equivalent to random guessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "roc(dummy_model.predict_proba(X_test)[:,1], ax=axes[0])\n",
    "conf_matrix(dummy_preds, ax=axes[1])\n",
    "pr_curve(dummy_model.predict_proba(X_test)[:,1], ax=axes[2])\n",
    "plt.suptitle('Dummy Classifier with Stratified Strategy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummy classifier has 0 precision, recall, and $F_1$ score for the positive class (attacker):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report(dummy_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "Naive Bayes makes a naive assumption of conditional independence of the features to simplify the use of Bayes theorem for modeling. It is very fast to train and a good baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB # build another baseline with Naive Bayes\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "nb_pipeline = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('nb', GaussianNB())\n",
    "]).fit(X_train, y_train)\n",
    "nb_preds = nb_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior probabilities will be the percentage of each class in the data. 99.91% for valid users and 0.09% for attackers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipeline.named_steps['nb'].class_prior_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model correctly flags a few of the attackers, but it has a ton of false positives. Note that this is better than the dummy model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "roc(nb_pipeline.predict_proba(X_test)[:,1], ax=axes[0])\n",
    "conf_matrix(nb_preds, ax=axes[1])\n",
    "pr_curve(nb_pipeline.predict_proba(X_test)[:,1], ax=axes[2])\n",
    "plt.suptitle('Naive Bayes Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better than the dummy classifier, but it doesn't meet our stakeholders' requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report(nb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "We will be maximizing recall (averaged across the classes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('lr', LogisticRegression(random_state=0))\n",
    "])\n",
    "\n",
    "search_space = {\n",
    "    'lr__C': [0.1, 0.5, 1, 2]\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    lr_pipeline, search_space, scoring='recall_macro', cv=5\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "lr_preds = lr_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we knew how much our stakeholders valued recall over precision. We could then make a custom F-beta scorer to use with grid search. Here is a scorer that values recall 5 times more than precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "f_five = make_scorer(fbeta_score, beta=5) # pass as the `scoring` argument to grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization selected from the grid search is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model had no false positives and is much better than the baselines. The ROC curve is much closer to the top-left corner, and the precision-recall is much closer to the top-right corner. Notice that the ROC curve is a bit more optimistic of the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "roc(lr_grid.predict_proba(X_test)[:,1], ax=axes[0])\n",
    "conf_matrix(lr_preds, ax=axes[1])\n",
    "pr_curve(lr_grid.predict_proba(X_test)[:,1], ax=axes[2])\n",
    "plt.suptitle('Logistic Regression Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model meets the requirements of the SOC. Our recall is at least 70% and precision is at least 85%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report(lr_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is a problem. We can't update this model with additional labeled data. It has to be trained from scratch to add data. We will see how to support this in the next notebook ([`5-online_learning.ipynb`](./5-online_learning.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div style=\"overflow: hidden; margin-bottom: 10px;\">\n",
    "    <div style=\"float: left;\">\n",
    "        <a href=\"./3-EDA_labeled_data.ipynb\">\n",
    "            <button>&#8592; Previous Notebook</button>\n",
    "        </a>\n",
    "    </div>\n",
    "    <div style=\"float: right;\">\n",
    "        <a href=\"./5-online_learning.ipynb\">\n",
    "            <button>Next Notebook &#8594;</button>\n",
    "        </a>\n",
    "    </div>\n",
    "</div>\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
