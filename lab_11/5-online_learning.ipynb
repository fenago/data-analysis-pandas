{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Learning\n",
    "We need to be able to continuously update our model and adapt to changes in the behavior of the attackers. To do so, we will be building an online learning model.\n",
    "\n",
    "## Setup\n",
    "Import the packages we need and read in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "\n",
    "with sqlite3.connect('logs/logs.db') as conn:\n",
    "    logs_2018 = pd.read_sql(\n",
    "        'SELECT * FROM logs WHERE datetime BETWEEN \"2018-01-01\" AND \"2019-01-01\";', \n",
    "        conn, parse_dates=['datetime'], index_col='datetime'\n",
    "    )\n",
    "    hackers_2018 = pd.read_sql(\n",
    "        'SELECT * FROM attacks WHERE start BETWEEN \"2018-01-01\" AND \"2019-01-01\";', \n",
    "        conn, parse_dates=['start', 'end']\n",
    "    ).assign(\n",
    "        start_floor=lambda x: x.start.dt.floor('min'),\n",
    "        end_ceil=lambda x: x.end.dt.ceil('min')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our functions for getting the X and y for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X(log, day):\n",
    "    \"\"\"\n",
    "    Get data we can use for the X\n",
    "    \n",
    "    Parameters:\n",
    "        - log: The logs dataframe\n",
    "        - day: A day or single value we can use as a datetime index slice\n",
    "    \n",
    "    Returns: \n",
    "        A `pandas.DataFrame` object\n",
    "    \"\"\"\n",
    "    return pd.get_dummies(log.loc[day].assign(\n",
    "        failures=lambda x: 1 - x.success\n",
    "    ).query('failures > 0').resample('1min').agg(\n",
    "        {'username': 'nunique', 'failures': 'sum'}\n",
    "    ).dropna().rename(\n",
    "        columns={'username': 'usernames_with_failures'}\n",
    "    ).assign(\n",
    "        day_of_week=lambda x: x.index.dayofweek, \n",
    "        hour=lambda x: x.index.hour\n",
    "    ).drop(columns=['failures']), columns=['day_of_week', 'hour'])\n",
    "\n",
    "def get_y(datetimes, hackers, resolution='1min'):\n",
    "    \"\"\"\n",
    "    Get data we can use for the y (whether or not a hacker attempted a log in during that time).\n",
    "    \n",
    "    Parameters:\n",
    "        - datetimes: The datetimes to check for hackers\n",
    "        - hackers: The dataframe indicating when the attacks started and stopped\n",
    "        - resolution: The granularity of the datetime. Default is 1 minute.\n",
    "        \n",
    "    Returns:\n",
    "        `pandas.Series` of Booleans.\n",
    "    \"\"\"\n",
    "    date_ranges = hackers.apply(\n",
    "        lambda x: pd.date_range(x.start_floor, x.end_ceil, freq=resolution), \n",
    "        axis=1\n",
    "    )\n",
    "    dates = pd.Series(dtype='object')\n",
    "    for date_range in date_ranges:\n",
    "        dates = pd.concat([dates, date_range.to_series()])\n",
    "    return datetimes.isin(dates)\n",
    "\n",
    "def get_X_y(log, day, hackers):\n",
    "    \"\"\"\n",
    "    Get the X, y data to build a model with.\n",
    "    \n",
    "    Parameters:\n",
    "        - log: The logs dataframe\n",
    "        - day: A day or single value we can use as a datetime index slice\n",
    "        - hackers: The dataframe indicating when the attacks started and stopped\n",
    "        \n",
    "    Returns:\n",
    "        X, y tuple where X is a `pandas.DataFrame` object\n",
    "        and y is a `pandas.Series` object\n",
    "    \"\"\"\n",
    "    X = get_X(log, day)\n",
    "    y = get_y(X.reset_index().datetime, hackers)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with the 2018 data to initially train our model, and then move to a monthly frequency to predict (and evaluate) each month and update the model afterwards (but before checking the next month)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2018, y_2018 = get_X_y(logs_2018, '2018', hackers_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Classification\n",
    "Using SGD, we will be able to build a logistic regression model that can be updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ml_utils.partial_fit_pipeline import PartialFitPipeline\n",
    "\n",
    "model = PartialFitPipeline([\n",
    "    ('scale', StandardScaler()), \n",
    "    ('sgd', SGDClassifier(\n",
    "        random_state=10, max_iter=1000, tol=1e-3, loss='log', \n",
    "        average=1000, learning_rate='adaptive', eta0=0.01\n",
    "    ))\n",
    "]).fit(X_2018, y_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can look at the coefficients for each of our features. The largest coefficient is `usernames_with_failures`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(col, coef) for col, coef in zip(X_2018.columns, model.named_steps['sgd'].coef_[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict, Evaluate, and Update Model\n",
    "\n",
    "#### Step 1: Gather test data\n",
    "We will use January 2019:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect('logs/logs.db') as conn:\n",
    "    logs_2019 = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT * \n",
    "        FROM logs \n",
    "        WHERE datetime BETWEEN \"2019-01-01\" AND \"2020-01-01\";\n",
    "        \"\"\", conn, parse_dates=['datetime'], index_col='datetime'\n",
    "    )\n",
    "    hackers_2019 = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT * \n",
    "        FROM attacks \n",
    "        WHERE start BETWEEN \"2019-01-01\" AND \"2020-01-01\";\n",
    "        \"\"\", conn, parse_dates=['start', 'end']\n",
    "    ).assign(\n",
    "        start_floor=lambda x: x.start.dt.floor('min'),\n",
    "        end_ceil=lambda x: x.end.dt.ceil('min')\n",
    "    )\n",
    "\n",
    "X_jan, y_jan = get_X_y(logs_2019, '2019-01', hackers_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Evaluate Performance\n",
    "Recall is too low here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_jan, model.predict(X_jan)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize where we are on the ROC and precision-recall curves. Let's make a convenience function since we will be plotting these often:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils.classification import confusion_matrix_visual, plot_pr_curve, plot_roc\n",
    "\n",
    "def plot_performance(model, X, y, threshold=None, title=None, show_target=True):\n",
    "    \"\"\"\n",
    "    Plot the ROC, confusion matrix, and precision-recall curve side-by-side.\n",
    "    \n",
    "    Parameters:\n",
    "        - model: The model object to use for prediction.\n",
    "        - X: The features to pass in for prediction from the testing set.\n",
    "        - y: The actuals to evaluate the prediction.\n",
    "        - threshold: Value to use as threshold when predicting probabilities.\n",
    "        - title: A title for the subplots.\n",
    "        - show_target: Whether to show the target regions on the ROC/PR curve.\n",
    "        \n",
    "    Returns:\n",
    "        Matplotlib `Axes` object.    \n",
    "    \"\"\"\n",
    "    # make the subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    # plot each visualization\n",
    "    plot_roc(y, model.predict_proba(X)[:,1], ax=axes[0])\n",
    "    confusion_matrix_visual(\n",
    "        y, model.predict_proba(X)[:,1] >= (threshold or 0.5), \n",
    "        class_labels=[False, True], ax=axes[1]\n",
    "    )\n",
    "    plot_pr_curve(y, model.predict_proba(X)[:,1], ax=axes[2])\n",
    "\n",
    "    # show the target regions if desired\n",
    "    if show_target:\n",
    "        axes[0].axvspan(0, 0.1, color='lightgreen', alpha=0.5)\n",
    "        axes[0].axhspan(0.7, 1, color='lightgreen', alpha=0.5)\n",
    "        axes[0].annotate(\n",
    "            'region with acceptable\\nFPR and TPR', \n",
    "            xy=(0.1, 0.7), xytext=(0.17, 0.65), \n",
    "            arrowprops=dict(arrowstyle='->')\n",
    "        )\n",
    "\n",
    "        axes[2].axvspan(0.7, 1, color='lightgreen', alpha=0.5)\n",
    "        axes[2].axhspan(0.85, 1, color='lightgreen', alpha=0.5)\n",
    "        axes[2].annotate(\n",
    "            'region with acceptable\\nprecision and recall', \n",
    "            xy=(0.7, 0.85), xytext=(0.3, 0.6), \n",
    "            arrowprops=dict(arrowstyle='->')\n",
    "        )\n",
    "\n",
    "        # mark the current performance\n",
    "        tn, fn, fp, tp = [int(x.get_text()) for x in axes[1].texts]\n",
    "        precision, recall, fpr = tp / (tp + fp), tp / (tp + fn), fp / (fp + tn)\n",
    "\n",
    "        prefix = 'current performance' if not threshold else f'chosen threshold: {threshold:.2%}'\n",
    "\n",
    "        axes[0].annotate(\n",
    "            f'{prefix}\\n- FPR={fpr:.2%}\\n- TPR={recall:.2%}', \n",
    "            xy=(fpr, recall), xytext=(0.05, 0.45), \n",
    "            arrowprops=dict(arrowstyle='->')\n",
    "        )\n",
    "        axes[2].annotate(\n",
    "            f'{prefix}\\n- precision={precision:.2%}\\n- recall={recall:.2%}', \n",
    "            xy=(recall, precision), xytext=(0.2, 0.85), \n",
    "            arrowprops=dict(arrowstyle='->')\n",
    "        )\n",
    "\n",
    "    # show the title if specified\n",
    "    if title:\n",
    "        plt.suptitle(title)\n",
    "\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we are not quite where we need to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plot_performance(\n",
    "    model, X_jan, y_jan, \n",
    "    title='Stochastic Gradient Descent Classifier '\\\n",
    "        '(Tested on January 2019 Data)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try picking a custom threshold based on acceptable TPR and FPR\n",
    "We will be using the threshold with the precision-recall curve, but here is how to get it from the ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils.classification import find_threshold_roc\n",
    "\n",
    "threshold = find_threshold_roc(\n",
    "    y_jan, model.predict_proba(X_jan)[:,1], \n",
    "    fpr_below=0.1, tpr_above=0.7\n",
    ").max()\n",
    "threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try picking a custom threshold based on acceptable recall and precision\n",
    "Note this is actually the same threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils.classification import find_threshold_pr\n",
    "\n",
    "threshold = find_threshold_pr(\n",
    "    y_jan, model.predict_proba(X_jan)[:,1], \n",
    "    min_precision=0.85, min_recall=0.7\n",
    ").max()\n",
    "threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this threshold, we are able to move into the acceptable region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plot_performance(\n",
    "    model, X_jan, y_jan, threshold=threshold, \n",
    "    title='Stochastic Gradient Descent Classifier '\\\n",
    "        '(Tested on January 2019 Data)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Update the model to include true labels for previously predicted test data (Jan 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update\n",
    "model.partial_fit(X_jan, y_jan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Repeat process (this time we use Feb 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feb, y_feb = get_X_y(logs_2019, '2019-02', hackers_2019)\n",
    "\n",
    "print(classification_report(y_feb, model.predict_proba(X_feb)[:,1] >= threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the model is better and now a larger section of both the ROC curve and the precision-recall curve fall in the acceptable performance region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plot_performance(\n",
    "    model, X_feb, y_feb, threshold=threshold,\n",
    "    title='Stochastic Gradient Descent Classifier '\\\n",
    "        '(Tested on February 2019 Data)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting our model to the test\n",
    "In the time that we've been building all these models, March 2019 has come and gone. Our stakeholders are done waiting around for us and want results. Let's show our performance.\n",
    "\n",
    "### Step 1: Update with the February 2019 data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.partial_fit(X_feb, y_feb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Predict the March data for the stakeholders and send it off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_march, y_march = get_X_y(logs_2019, '2019-03', hackers_2019)\n",
    "march_2019_preds = model.predict_proba(X_march)[:,1] >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluation\n",
    "The subject area experts examined our predictions and returned the following after checking our predictions. We are meeting our metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_march, march_2019_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to adapt to new data and still meet our performance requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plot_performance(\n",
    "    model, X_march, y_march, threshold=threshold,\n",
    "    title='Stochastic Gradient Descent Classifier '\\\n",
    "        '(Tested on March 2019 Data)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div style=\"overflow: hidden; margin-bottom: 10px;\">\n",
    "    <div style=\"float: left;\">\n",
    "        <a href=\"./4-supervised_anomaly_detection.ipynb\">\n",
    "            <button>&#8592; Previous Notebook</button>\n",
    "        </a>\n",
    "    </div>\n",
    "    <div style=\"float: right;\">\n",
    "        <a href=\"../../solutions/ch_11/exercise_1.ipynb\">\n",
    "            <button>Solutions</button>\n",
    "        </a>\n",
    "        <a href=\"../../ch_12/README.md\">\n",
    "            <button>Chapter 12 &#8594;</button>\n",
    "        </a>\n",
    "    </div>\n",
    "</div>\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
