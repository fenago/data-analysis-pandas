{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating data with pandas and numpy\n",
    "\n",
    "## About the Data\n",
    "In this notebook, we will be working with 2 datasets:\n",
    "- Facebook's stock price throughout 2018 (obtained using the [`stock_analysis` package](https://github.com/fenago/stock-analysis)).\n",
    "- daily weather data for NYC from the [National Centers for Environmental Information (NCEI) API](https://www.ncdc.noaa.gov/cdo-web/webservices/v2).\n",
    "\n",
    "*Note: The NCEI is part of the National Oceanic and Atmospheric Administration (NOAA) and, as you can see from the URL for the API, this resource was created when the NCEI was called the NCDC. Should the URL for this resource change in the future, you can search for \"NCEI weather API\" to find the updated one.*\n",
    "\n",
    "## Background on the weather data\n",
    "\n",
    "Data meanings:\n",
    "- `AWND`: average wind speed\n",
    "- `PRCP`: precipitation in millimeters\n",
    "- `SNOW`: snowfall in millimeters\n",
    "- `SNWD`: snow depth in millimeters\n",
    "- `TMAX`: maximum daily temperature in Celsius\n",
    "- `TMIN`: minimum daily temperature in Celsius\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fb = pd.read_csv('data/fb_2018.csv', index_col='date', parse_dates=True).assign(\n",
    "    trading_volume=lambda x: pd.cut(x.volume, bins=3, labels=['low', 'med', 'high'])\n",
    ")\n",
    "fb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('data/weather_by_station.csv', index_col='date', parse_dates=True)\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into any calculations, let's make sure `pandas` won't put things in scientific notation. We will modify how floats are formatted for displaying. The format we will apply is `.2f`, which will provide the float with 2 digits after the decimal point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing DataFrames\n",
    "We learned about `agg()` in the [`2-dataframe_operations.ipynb`](./2-dataframe_operations.ipynb) notebook when we learned about window calculations; however, we can call this on the dataframe directly to aggregate its contents into a single series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.agg({\n",
    "    'open': np.mean, \n",
    "    'high': np.max, \n",
    "    'low': np.min, \n",
    "    'close': np.mean, \n",
    "    'volume': np.sum\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to find the total snowfall and precipitation recorded in Central Park in 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.query('station == \"GHCND:USW00094728\"')\\\n",
    "    .pivot(columns='datatype', values='value')[['SNOW', 'PRCP']]\\\n",
    "    .sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to passing `'sum'` to `agg()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.query('station == \"GHCND:USW00094728\"')\\\n",
    "    .pivot(columns='datatype', values='value')[['SNOW', 'PRCP']]\\\n",
    "    .agg('sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we aren't limited to providing a single aggregation per column. We can pass a list, and we will get a dataframe back instead of a series. Null values are placed where we don't have a calculation result to display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.agg({\n",
    "    'open': 'mean',\n",
    "    'high': ['min', 'max'],\n",
    "    'low': ['min', 'max'],\n",
    "    'close': 'mean'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `groupby()`\n",
    "Often we won't want to aggregate on the entire dataframe, but on groups within it. For this purpose, we can run `groupby()` before the aggregation. If we group by the `trading_volume` column, we will get a row for each of the values it takes on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.groupby('trading_volume').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we call `groupby()`, we can still select columns for aggregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.groupby('trading_volume')['close'].agg(['min', 'max', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still provide a dictionary specifying the aggregations to perform, but passing a list for a column will result in a hierarchical index for the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_agg = fb.groupby('trading_volume').agg({\n",
    "    'open': 'mean',\n",
    "    'high': ['min', 'max'],\n",
    "    'low': ['min', 'max'],\n",
    "    'close': 'mean'\n",
    "})\n",
    "fb_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hierarchical index in the columns looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_agg.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a list comprehension, we can join the levels (in a tuple) with an `_` at each iteration: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_agg.columns = ['_'.join(col_agg) for col_agg in fb_agg.columns]\n",
    "fb_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can group on values in the index if we tell `groupby()`, which `level` to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.loc['2018-10'].query('datatype == \"PRCP\"')\\\n",
    "    .groupby(level=0).mean().head().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a `Grouper` object, which can also roll up the datetimes in the index. Here, we find the quarterly total precipitation per station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.query('datatype == \"PRCP\"').groupby(\n",
    "    ['station_name', pd.Grouper(freq='Q')]\n",
    ").sum().unstack().sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use `filter()` to exclude some groups from aggregation. Here, we only keep groups with names ending in \"NY US\" in the group's `name` attribute, which is the station name in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.groupby('station_name').filter( # station names with \"NY US\" in them\n",
    "    lambda x: x.name.endswith('NY US')\n",
    ").query('datatype == \"SNOW\"').groupby('station_name').sum().squeeze() # aggregate and make a series (squeeze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which months have the most precipitation. First, we need to group by day and average the precipitation across the stations. Then we can group by month and sum the resulting precipitation. We use `nlargest()` to give the 5 months with the most precipitation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.query('datatype == \"PRCP\"')\\\n",
    "    .groupby(level=0).mean()\\\n",
    "    .groupby(pd.Grouper(freq='M')).sum().value.nlargest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the previous result was surprising. The saying goes \"April showers bring May flowers\"; yet April wasn't in the top 5 (neither was May for that matter). Snow will count towards precipitation, but that doesn't explain why summer months are higher than April. Let's look for days that accounted for a large percentage of the precipitation in a given month. \n",
    "\n",
    "In order to do so, we need to calculate the average daily precipitation across stations and then find the total per month. This will be the denominator. However, in order to divide the daily values by the total for their month, we will need a series of equal dimensions. This means we will need to use `transform()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.query('datatype == \"PRCP\"')\\\n",
    "    .rename(dict(value='prcp'), axis=1)\\\n",
    "    .groupby(level=0).mean()\\\n",
    "    .groupby(pd.Grouper(freq='M'))\\\n",
    "    .transform(np.sum)['2018-01-28':'2018-02-03']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have the same value repeated for each day in the month it belongs to. This will allow us to calculate the percentage of the monthly precipitation that occurred each day and then pull out the largest values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather\\\n",
    "    .query('datatype == \"PRCP\"')\\\n",
    "    .rename(dict(value='prcp'), axis=1)\\\n",
    "    .groupby(level=0).mean()\\\n",
    "    .assign(\n",
    "        total_prcp_in_month=lambda x: \\\n",
    "            x.groupby(pd.Grouper(freq='M')).transform(np.sum),\n",
    "        pct_monthly_prcp=lambda x: \\\n",
    "            x.prcp.div(x.total_prcp_in_month)\n",
    "    )\\\n",
    "    .nlargest(5, 'pct_monthly_prcp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transform()` can be used on dataframes as well. We can use it to easily standardize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb[['open', 'high', 'low', 'close']]\\\n",
    "    .transform(lambda x: (x - x.mean()).div(x.std()))\\\n",
    "    .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot tables and crosstabs\n",
    "We saw pivots in [`lab_08/4-reshaping_data.ipynb`](../lab_08/4-reshaping_data.ipynb); however, we weren't able to provide any aggregations. With `pivot_table()`, we get the mean by default. In its simplest form, we provide a column to place along the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.pivot_table(columns='trading_volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By placing the trading volume in the index, we get the transpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.pivot_table(index='trading_volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `pivot()`, we also weren't able to handle multi-level indices or indices with repeated values. For this reason we haven't been able to put the weather data in the wide format. The `pivot_table()` method solves this issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.reset_index().pivot_table(\n",
    "    index=['date', 'station', 'station_name'], \n",
    "    columns='datatype', \n",
    "    values='value',\n",
    "    aggfunc='median'\n",
    ").reset_index().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `pd.crosstab()` function to create a frequency table. For example, if we want to see how many low-, medium-, and high-volume trading days Facebook stock had each month, we can use crosstab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    index=fb.trading_volume,\n",
    "    columns=fb.index.month,\n",
    "    colnames=['month'] # name the columns index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can normalize with the row or column totals with the `normalize` parameter. This shows percentage of the total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    index=fb.trading_volume,\n",
    "    columns=fb.index.month,\n",
    "    colnames=['month'],\n",
    "    normalize='columns'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to perform a calculation other than counting the frequency, we can pass the column to run the calculation on to `values` and the function to use to `aggfunc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    index=fb.trading_volume,\n",
    "    columns=fb.index.month,\n",
    "    colnames=['month'],\n",
    "    values=fb.close,\n",
    "    aggfunc=np.mean\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get row and column subtotals with the `margins` parameter. Let's count the number of times each station recorded snow per month and include the subtotals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_data = weather.query('datatype == \"SNOW\"')\n",
    "pd.crosstab(\n",
    "    index=snow_data.station_name,\n",
    "    columns=snow_data.index.month,\n",
    "    colnames=['month'],\n",
    "    values=snow_data.value,\n",
    "    aggfunc=lambda x: (x > 0).sum(),\n",
    "    margins=True, # show row and column subtotals\n",
    "    margins_name='total observations of snow' # name the subtotals\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div>\n",
    "    <a href=\"./2-dataframe_operations.ipynb\">\n",
    "        <button>&#8592; Previous Notebook</button>\n",
    "    </a>\n",
    "    <a href=\"./4-time_series.ipynb\">\n",
    "        <button style=\"float: right;\">Next Notebook &#8594;</button>\n",
    "    </a>\n",
    "</div>\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
