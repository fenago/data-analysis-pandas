{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification by Wine Type, Part 2\n",
    "\n",
    "## Wine Data\n",
    "Data from http://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
    "\n",
    "### Citations\n",
    "<pre>\n",
    "Dua, D. and Karra Taniskidou, E. (2017). \n",
    "UCI Machine Learning Repository [http://archive.ics.uci.edu/ml/index.php]. \n",
    "Irvine, CA: University of California, School of Information and Computer Science.\n",
    "</pre>\n",
    "\n",
    "<pre>\n",
    "P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n",
    "Modeling wine preferences by data mining from physicochemical properties.\n",
    "In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n",
    "</pre>\n",
    "\n",
    "Available at:\n",
    "- [@Elsevier](http://dx.doi.org/10.1016/j.dss.2009.05.016)\n",
    "- [Pre-press (pdf)](http://www3.dsi.uminho.pt/pcortez/winequality09.pdf)\n",
    "- [bib](http://www3.dsi.uminho.pt/pcortez/dss09.bib)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "red_wine = pd.read_csv('data/winequality-red.csv')\n",
    "white_wine = pd.read_csv('data/winequality-white.csv', sep=';')\n",
    "wine = pd.concat([\n",
    "    white_wine.assign(kind='white'), red_wine.assign(kind='red')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we completed our EDA in the [`wine.ipynb`](../ch_09/wine.ipynb) notebook for last chapter, we will just look at some rows to refresh our memory of the data rather than repeating the EDA here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.sample(5, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "As in chapter 9, we will try to predict if a wine is red or white based on its chemical properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine_y = np.where(wine.kind == 'red', 1, 0)\n",
    "wine_X = wine.drop(columns=['quality', 'kind'])\n",
    "\n",
    "w_X_train, w_X_test, w_y_train, w_y_test = train_test_split(\n",
    "    wine_X, wine_y, test_size=0.25, random_state=0, stratify=wine_y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classification of Red and White Wines from Chapter 9\n",
    "This was the result from chapter 9 for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "white_or_red = Pipeline([\n",
    "    ('scale', StandardScaler()), \n",
    "    ('lr', LogisticRegression(random_state=0))\n",
    "]).fit(w_X_train, w_y_train)\n",
    "\n",
    "kind_preds = white_or_red.predict(w_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performed very well without any tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(w_y_test, kind_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils.classification import plot_roc\n",
    "\n",
    "plot_roc(w_y_test, white_or_red.predict_proba(w_X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils.classification import confusion_matrix_visual\n",
    "\n",
    "confusion_matrix_visual(w_y_test, kind_preds, ['white', 'red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "### Variance Threshold\n",
    "Features with little to no variance don't contribute much to our classification model. We can remove those and work with the features that have some variance. By default, the `VarianceThreshold` class will remove all features that have the same value throughout the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "white_or_red_min_var = Pipeline([\n",
    "    ('feature_selection', VarianceThreshold(threshold=0.01)), # keep features with variance > 0.01\n",
    "    ('scale', StandardScaler()), \n",
    "    ('lr', LogisticRegression(random_state=0))\n",
    "]).fit(w_X_train, w_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which features got removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_X_train.columns[\n",
    "    ~white_or_red_min_var.named_steps[\n",
    "        'feature_selection'\n",
    "    ].get_support()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance doesn't change much when using just 9 out of the 11 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(\n",
    "    w_y_test, white_or_red_min_var.predict(w_X_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis\n",
    "Can we see a way to easily separate these that might help us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils.pca import pca_scatter\n",
    "pca_scatter(wine_X, wine_y, 'wine is red?')\n",
    "plt.title('Wine Kind PCA (2 components)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils.pca import pca_scatter_3d\n",
    "pca_scatter_3d(wine_X, wine_y, 'wine is red?', elev=20, azim=-10)\n",
    "plt.suptitle('Wine Type PCA (3 components)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many PCA components explain most of the variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from ml_utils.pca import pca_explained_variance_plot\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('normalize', MinMaxScaler()), ('pca', PCA(8, random_state=0))\n",
    "]).fit(w_X_train, w_y_train) \n",
    "\n",
    "pca_explained_variance_plot(pipeline.named_steps['pca'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a scree plot to find the number of components to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from ml_utils.pca import pca_scree_plot\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('normalize', MinMaxScaler()), ('pca', PCA(8, random_state=0))\n",
    "]).fit(w_X_train, w_y_train)\n",
    "\n",
    "pca_scree_plot(pipeline.named_steps['pca'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will a model fit on these components perform better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('normalize', MinMaxScaler()),\n",
    "    ('pca', PCA(4, random_state=0)),\n",
    "    ('lr', LogisticRegression(\n",
    "        class_weight='balanced', random_state=0\n",
    "    ))\n",
    "]).fit(w_X_train, w_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we only have 4 features (our PCA components):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.named_steps['lr'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the agreement between our new model and the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agreement with logistic regression alone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "cohen_kappa_score(kind_preds, pipeline.predict(w_X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is still good using dimensionality reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds = pipeline.predict(w_X_test)\n",
    "print(classification_report(w_y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils.classification import plot_roc\n",
    "plot_roc(w_y_test, pipeline.predict_proba(w_X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils.classification import confusion_matrix_visual\n",
    "confusion_matrix_visual(w_y_test, preds, ['low', 'high'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can a decision tree tell us what features are important?\n",
    "Decision trees give us feature importances. These sum up to 1 with the highest being more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=0).fit(w_X_train, w_y_train)\n",
    "pd.DataFrame([(col, coef) for col, coef in zip(\n",
    "    w_X_train.columns, dt.feature_importances_\n",
    ")], columns=['feature', 'importance']\n",
    ").set_index('feature').sort_values(\n",
    "    'importance', ascending=False\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the top 2 features\n",
    "Looking at the top 2 features, we can see if it is possible to separate red and white wine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=wine['total sulfur dioxide'], y=wine['chlorides'], hue=wine.kind, alpha=0.25)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Top 2 Features for Classification of Red and White Wines\n",
    "We can build a model with just these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "important_features = ['total sulfur dioxide', 'chlorides']\n",
    "X_train = w_X_train[important_features]\n",
    "X_test = w_X_test[important_features]\n",
    "\n",
    "white_or_red_top_features = Pipeline([\n",
    "    ('scale', StandardScaler()), \n",
    "    ('lr', LogisticRegression(random_state=0))\n",
    "]).fit(X_train, w_y_train)\n",
    "\n",
    "top_features_kind_preds = white_or_red_top_features.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice our performance is still good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(w_y_test, top_features_kind_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils.classification import plot_roc\n",
    "\n",
    "plot_roc(w_y_test, white_or_red_top_features.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils.classification import confusion_matrix_visual\n",
    "\n",
    "confusion_matrix_visual(w_y_test, top_features_kind_preds, ['white', 'red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the decision tree\n",
    "We can also visualize the decisions the tree made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "graphviz.Source(export_graphviz(\n",
    "    DecisionTreeClassifier(\n",
    "        max_depth=2, random_state=0\n",
    "    ).fit(w_X_train, w_y_train),\n",
    "    feature_names=w_X_train.columns\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without graphviz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(\n",
    "    DecisionTreeClassifier(\n",
    "        max_depth=2, random_state=0\n",
    "    ).fit(w_X_train, w_y_train),\n",
    "    feature_names=w_X_train.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis on Logistic Regression\n",
    "We can look at the incorrect predictions to get a better feel for our data and the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_probabilities = pd.DataFrame(\n",
    "    white_or_red.predict_proba(w_X_test), \n",
    "    columns=['prob_white', 'prob_red']\n",
    ").assign(\n",
    "    is_red=w_y_test == 1,\n",
    "    pred_white=lambda x: x.prob_white >= 0.5, \n",
    "    pred_red=lambda x: np.invert(x.pred_white),\n",
    "    correct=lambda x: (np.invert(x.is_red) & x.pred_white)\n",
    "                       | (x.is_red & x.pred_red)\n",
    ")\n",
    "\n",
    "prediction_probabilities.sample(5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of prediction confidence\n",
    "When our model is confident, it is usually right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.displot(\n",
    "    data=prediction_probabilities, x='prob_red',\n",
    "    rug=True, kde=True, bins=20, col='correct', \n",
    "    facet_kws={'sharey': False}\n",
    ")\n",
    "g.set_axis_labels('probability wine is red', None)\n",
    "plt.suptitle('Prediction Confidence', y=1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the incorrect classifications outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "incorrect = w_X_test.assign(is_red=w_y_test).iloc[prediction_probabilities.query('not correct').index]\n",
    "chemical_properties = [col for col in wine.columns if col not in ['quality', 'kind']]\n",
    "melted = wine.drop(columns='quality').melt(id_vars=['kind'])\n",
    "\n",
    "fig, axes = plt.subplots(math.ceil(len(chemical_properties) / 4), 4, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for prop, ax in zip(chemical_properties, axes):\n",
    "    sns.boxplot(\n",
    "        data=melted[melted.variable.isin([prop])], \n",
    "        x='variable', y='value', hue='kind', ax=ax,\n",
    "        palette={'white': 'lightyellow', 'red': 'orchid'}, \n",
    "        saturation=0.5, fliersize=2\n",
    "    ).set_xlabel('')\n",
    "    for _, wrong in incorrect.iterrows():\n",
    "        x_coord = -0.2 if not wrong['is_red'] else 0.2\n",
    "        ax.scatter(x_coord, wrong[prop], marker='x', color='red', s=50)\n",
    "    \n",
    "# remove the extra subplots\n",
    "for ax in axes[len(chemical_properties):]:\n",
    "    ax.remove()\n",
    "    \n",
    "plt.suptitle(\n",
    "    'Comparing Chemical Properties of Red and White Wines'\n",
    "    '\\n(classification errors are red x\\'s)'\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div style=\"overflow: hidden; margin-bottom: 10px;\">\n",
    "    <div style=\"float: left;\">\n",
    "        <a href=\"../../ch_09/wine.ipynb\">\n",
    "            <button>&#8592; Chapter 9</button>\n",
    "        </a>\n",
    "        <a href=\"./planets_ml.ipynb\">\n",
    "            <button>Planets</button>\n",
    "        </a>\n",
    "        <a href=\"./red_wine.ipynb\">\n",
    "            <button>Red Wine</button>\n",
    "        </a>\n",
    "    </div>\n",
    "    <div style=\"float: right;\">\n",
    "        <a href=\"../../solutions/ch_10/exercise_1.ipynb\">\n",
    "            <button>Solutions</button>\n",
    "        </a>\n",
    "        <a href=\"../ch_11/1-EDA_unlabeled_data.ipynb\">\n",
    "            <button>Chapter 11 &#8594;</button>\n",
    "        </a>\n",
    "    </div>\n",
    "</div>\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
