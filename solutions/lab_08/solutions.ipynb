{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "\n",
    "## About the Data\n",
    "In this notebook, we will be working with two data sources: \n",
    "- 2018 stock data for Facebook, Apple, Amazon, Netflix, and Google (obtained using the [`stock_analysis`](https://github.com/fenago/stock-analysis) package)\n",
    "- European Centre for Disease Prevention and Control's (ECDC) [daily number of new reported cases of COVID-19 by country worldwide dataset](https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide) collected on September 19, 2020 via [this link](https://opendata.ecdc.europa.eu/covid19/casedistribution/csv)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "We want to look at data for the FAANG stocks (Facebook, Apple, Amazon, Netflix, and Google), but we were given each as a separate CSV file. Make them into a single file and store the dataframe of the FAANG data as `faang`:\n",
    "1. Read each file in.\n",
    "2. Add a column to each dataframe indicating the ticker it is for.\n",
    "3. Append them together into a single dataframe.\n",
    "4. Save the result to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang = pd.DataFrame()\n",
    "for ticker in ['fb', 'aapl', 'amzn', 'nflx', 'goog']:\n",
    "    df = pd.read_csv(f'../../lab_08/exercises/{ticker}.csv')\n",
    "    # make the ticker the first column\n",
    "    df.insert(0, 'ticker', ticker.upper())\n",
    "    faang = faang.append(df)\n",
    "\n",
    "faang.to_csv('faang.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "With `faang`, use type conversion to change the `date` column to datetime and the `volume` column to integers. Then, sort by `date` and `ticker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang = faang.assign(\n",
    "    date=lambda x: pd.to_datetime(x.date),\n",
    "    volume=lambda x: x.volume.astype(int)\n",
    ").sort_values(\n",
    "    ['date', 'ticker']\n",
    ")\n",
    "\n",
    "faang.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Find the 7 rows with the lowest value for `volume`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang.nsmallest(7, 'volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Right now, the data is somewhere between long and wide format. Use `melt()` to make it completely long format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_faang = faang.melt(\n",
    "    id_vars=['ticker', 'date'], \n",
    "    value_vars=['open', 'high', 'low', 'close', 'volume']\n",
    ")\n",
    "melted_faang.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Suppose we found out there was a glitch in how the data was recorded on July 26, 2018. How should we handle this?\n",
    "\n",
    "> Given that this is a large data set (~ 1 year), we would be tempted to just drop that date and interpolate. However, some preliminary research on that date for the FAANG stocks reveals that FB took a huge tumble that day. If we had interpolated, we would have missed the magnitude of the drop.\n",
    "\n",
    "## Exercise 6\n",
    "The European Centre for Disease Prevention and Control (ECDC) provides an open dataset on COVID-19 cases called, [*daily number of new reported cases of COVID-19 by country worldwide*](https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide). This dataset is updated daily, but we will use a snapshot that contains data from January 1, 2020 through September 18, 2020. Clean and pivot the data so that it is in wide format:\n",
    "\n",
    "1. Read in the `covid19_cases.csv` file.\n",
    "2. Create a `date` column using the data in the `dateRep` column and the `pd.to_datetime()` function.\n",
    "3. Set the `date` column as the index and sort the index.\n",
    "4. Replace occurrences of `United_States_of_America` and `United_Kingdom` with `USA` and `UK`, respectively.\n",
    "5. Using the `countriesAndTerritories` column, filter the data down to Argentina, Brazil, China, Colombia, India, Italy, Mexico, Peru, Russia, Spain, Turkey, the UK, and the USA.\n",
    "6. Pivot the data so that the index contains the dates, the columns contain the country names, and the values are the case counts in the `cases` column. Be sure to fill in `NaN` values with `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid = pd.read_csv('../../lab_08/exercises/covid19_cases.csv').assign(\n",
    "    date=lambda x: pd.to_datetime(x.dateRep, format='%d/%m/%Y')\n",
    ").set_index('date').replace(\n",
    "    'United_States_of_America', 'USA'\n",
    ").replace('United_Kingdom', 'UK').sort_index()\n",
    "\n",
    "covid[\n",
    "    covid.countriesAndTerritories.isin([\n",
    "        'Argentina', 'Brazil', 'China', 'Colombia', 'India', 'Italy', \n",
    "        'Mexico', 'Peru', 'Russia', 'Spain', 'Turkey', 'UK', 'USA'\n",
    "    ])\n",
    "].reset_index().pivot(index='date', columns='countriesAndTerritories', values='cases').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "In order to determine the case totals per country efficiently, we need the aggregation skills we will learn in *Lab 4, Aggregating DataFrames*, so the ECDC data in the `covid19_cases.csv` file has been aggregated for us and saved in the `covid19_total_cases.csv` file. It contains the total number of case per country. Use this data to find the 20 countries with the largest COVID-19 case totals. Hints:\n",
    "\n",
    "- When reading in the CSV file, pass in `index_col='cases'`.\n",
    "- Note that it will be helpful to transpose the data before isolating the countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('../../lab_08/exercises/covid19_total_cases.csv', index_col='index')\\\n",
    "    .T.nlargest(20, 'cases').sort_values('cases', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
