{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "\n",
    "## About the Data\n",
    "In this notebook, we will be working with 3 datasets:\n",
    "- 2018 stock data for Facebook, Apple, Amazon, Netflix, and Google (obtained using the [`stock_analysis` package](https://github.com/fenago/stock-analysis)) and earthquake data from the USGS API.\n",
    "- Earthquake data from September 18, 2018 - October 13, 2018 (obtained from the US Geological Survey (USGS) using the [USGS API](https://earthquake.usgs.gov/fdsnws/event/1/))\n",
    "- European Centre for Disease Prevention and Control's (ECDC) [daily number of new reported cases of COVID-19 by country worldwide dataset](https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide) collected on September 19, 2020 via [this link](https://opendata.ecdc.europa.eu/covid19/casedistribution/csv)\n",
    "\n",
    "## Setup\n",
    "Note that the COVID-19 data will be read in later as part of the solution to exercise 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "quakes = pd.read_csv('../../lab_09/exercises/earthquakes.csv')\n",
    "faang = pd.read_csv('../../lab_09/exercises/faang.csv', index_col='date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "With the `exercises/earthquakes.csv` file, select all the earthquakes in Japan with a of 4.9 or greater using the `mb` magnitude type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quakes.query(\n",
    "    \"parsed_place == 'Japan' and magType == 'mb' and mag >= 4.9\"\n",
    ")[['mag', 'magType', 'place']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Create bins for each full number of magnitude (for example, the first bin is (0, 1], the second is (1, 2], and so on) with the `ml` magnitude type and count how many are in each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quakes.query(\"magType == 'ml'\").assign(\n",
    "    mag_bin=lambda x: pd.cut(x.mag, np.arange(0, 10))\n",
    ").mag_bin.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Using the `exercises/faang.csv` file, group by the ticker and resample to monthly frequency. Aggregate the open and close prices with the mean, the high price with the max, the low price with the min, and the volume with the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang.groupby('ticker').resample('1M').agg(\n",
    "    {\n",
    "        'open': np.mean,\n",
    "        'high': np.max,\n",
    "        'low': np.min,\n",
    "        'close': np.mean,\n",
    "        'volume': np.sum\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Build a crosstab with the earthquake data between the `tsunami` column and the `magType` column. Rather than showing the frequency count, show the maximum magnitude that was observed for each combination. Put the magnitude type along the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(quakes.tsunami, quakes.magType, values=quakes.mag, aggfunc='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Calculate the rolling 60-day aggregations of the OHLC data by ticker for the FAANG data. Use the same aggregations as exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang.groupby('ticker').rolling('60D').agg(\n",
    "    {\n",
    "        'open': np.mean,\n",
    "        'high': np.max,\n",
    "        'low': np.min,\n",
    "        'close': np.mean,\n",
    "        'volume': np.sum\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "Create a pivot table of the FAANG data that compares the stocks. Put the ticker in the rows and and show the averages of the OHLC and volume traded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang.pivot_table(index='ticker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "Calculate the Z-scores of Amazon's data (ticker: AMZN) using `apply()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang.loc['2018-Q4'].query(\"ticker == 'AMZN'\").drop(columns='ticker').apply(\n",
    "    lambda x: x.sub(x.mean()).div(x.std())\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "Adding event descriptions:\n",
    "1. Create a dataframe with three columns: `ticker`, `date`, and `event`.\n",
    "    1. `ticker` will be `'FB'`.\n",
    "    2. `date` will be datetimes `['2018-07-25', '2018-03-19', '2018-03-20']`\n",
    "    3. `event` will be `['Disappointing user growth announced after close.', 'Cambridge Analytica story', 'FTC investigation']`.\n",
    "2. Set the index to `['date', 'ticker']`\n",
    "3. Merge this data to the FAANG data with a outer join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.DataFrame({\n",
    "    'ticker': 'FB',\n",
    "    'date': pd.to_datetime(\n",
    "         ['2018-07-25', '2018-03-19', '2018-03-20']\n",
    "    ), \n",
    "    'event': [\n",
    "         'Disappointing user growth announced after close.',\n",
    "         'Cambridge Analytica story',\n",
    "         'FTC investigation'\n",
    "    ]\n",
    "}).set_index(['date', 'ticker'])\n",
    "\n",
    "faang.reset_index().set_index(['date', 'ticker']).join(\n",
    "    events, how='outer'\n",
    ").sample(10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "Use the `transform()` method on the FAANG data to represent all the values in terms of the first date in the data. To do so, divide all values for each ticker by the values of the first date in the data for that ticker. This is referred to as an index, and the data for the first date is the base. [More information](https://ec.europa.eu/eurostat/statistics-explained/index.php/Beginners:Statistical_concept_-_Index_and_base_year). When data is in this format, we can easily see growth over time. Hint: `transform()` can take a function name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang = faang.reset_index().set_index(['ticker', 'date'])\n",
    "faang_index = (faang / faang.groupby(level='ticker').transform('first'))\n",
    "\n",
    "# view 3 rows of the result per ticker\n",
    "faang_index.groupby(level='ticker').agg('head', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10\n",
    "## Part 1\n",
    "1. Read in the data in the `exercises/covid19_cases.csv` file\n",
    "2. Create a `date` column by parsing the `dateRep` column into a datetime\n",
    "3. Set the `date` column as the index\n",
    "4. Use the `replace()` method to update all occurrences of `United_States_of_America` and `United Kingdom` to `USA` and `UK`, respectively\n",
    "5. Sort the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid = pd.read_csv('../../lab_09/exercises/covid19_cases.csv')\\\n",
    "    .assign(date=lambda x: pd.to_datetime(x.dateRep, format='%d/%m/%Y'))\\\n",
    "    .set_index('date')\\\n",
    "    .replace('United_States_of_America', 'USA')\\\n",
    "    .replace('United_Kingdom', 'UK')\\\n",
    "    .sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "For the 5 countries with the most cases (cumulative), find the day with the largest number of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_five_countries = covid\\\n",
    "    .groupby('countriesAndTerritories').cases.sum()\\\n",
    "    .nlargest(5).index\n",
    "\n",
    "covid[covid.countriesAndTerritories.isin(top_five_countries)]\\\n",
    "    .groupby('countriesAndTerritories').cases.idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "Find the 7-day average change in COVID-19 cases for the last week in the data for the countries found in part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid\\\n",
    "    .groupby(['countriesAndTerritories', pd.Grouper(freq='1D')]).cases.sum()\\\n",
    "    .unstack(0).diff().rolling(7).mean().last('1W')[top_five_countries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "Find the first date that each country other than China had cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid.reset_index()\\\n",
    "    .pivot(index='date', columns='countriesAndTerritories', values='cases')\\\n",
    "    .drop(columns='China')\\\n",
    "    .fillna(0)\\\n",
    "    .apply(lambda x: x[(x > 0)].idxmin())\\\n",
    "    .sort_values()\\\n",
    "    .rename(lambda x: x.replace('_', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5\n",
    "Rank the countries by total cases using percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid\\\n",
    "    .pivot_table(columns='countriesAndTerritories', values='cases', aggfunc='sum')\\\n",
    "    .T\\\n",
    "    .transform('rank', method='max', pct=True)\\\n",
    "    .sort_values('cases', ascending=False)\\\n",
    "    .rename(lambda x: x.replace('_', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
